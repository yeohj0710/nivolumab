{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# torchdiffeq 라이브러리 설치 후 사용 (pip install torchdiffeq)\n",
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 1. 데이터셋 정의\n",
    "#############################\n",
    "\n",
    "class PKPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    정적 특성(\"BW\", \"EGFR\", \"SEX\", \"RAAS\", \"BPS\", \"amt\", \"II\")와\n",
    "    시간(time), CP 값을 반환하며, 투여 이벤트는 amt와 II를 이용해 생성함.\n",
    "    단, 단일 Parquet 파일(\"../processed_data.parquet\")에서 각 환자별 데이터를 읽어옴.\n",
    "    max_files: 전체 데이터 중 사용할 행 수 제한 (즉, 몇 개의 환자 데이터를 사용할지)\n",
    "    max_rows: 각 샘플의 CP 시퀀스 길이 제한 (즉, CP 값의 몇 번째 값까지만 사용할지)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parquet_path=\"../processed_data.parquet\", transform=None, max_files=None, max_rows=None):\n",
    "        # 단일 Parquet 파일에서 데이터프레임 로드\n",
    "        self.df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "        # max_files가 지정되어 있으면 해당 행까지만 사용\n",
    "        if max_files is not None:\n",
    "            self.df = self.df.iloc[:max_files]\n",
    "        self.transform = transform\n",
    "        self.max_rows = max_rows\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # 정적 특성 추출 (dtype: float32)\n",
    "        static_features = row[[\"BW\", \"EGFR\", \"SEX\", \"RAAS\", \"BPS\", \"amt\", \"II\"]].astype(np.float32).values\n",
    "        static_features = torch.tensor(static_features)\n",
    "\n",
    "        # t=1부터 시작하는 CP 시퀀스 추출 (t=0은 CP=0이므로 저장하지 않음)\n",
    "        cp_sequence = np.array(row[\"CP_sequence\"], dtype=np.float32)\n",
    "        # max_rows가 지정되어 있으면 cp_sequence를 해당 길이까지만 사용\n",
    "        if self.max_rows is not None:\n",
    "            cp_sequence = cp_sequence[:self.max_rows]\n",
    "        t_values = np.arange(0, len(cp_sequence), dtype=np.float32)\n",
    "        t = torch.tensor(t_values)\n",
    "        cp = torch.tensor(cp_sequence, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # amt와 II를 이용해 투여 이벤트 생성\n",
    "        amt = static_features[5].item()\n",
    "        II = static_features[6].item()\n",
    "        total_time = t[-1].item()\n",
    "        injection_times = np.arange(0, total_time + 1e-3, II, dtype=np.float32)\n",
    "        injection_times = torch.tensor(injection_times, dtype=torch.float32)\n",
    "        injection_doses = torch.full(injection_times.shape, amt, dtype=torch.float32)\n",
    "\n",
    "        sample = {\n",
    "            \"static_features\": static_features,   # (7,)\n",
    "            \"t\": t,                               # (T,)\n",
    "            \"cp\": cp,                             # (T, 1)\n",
    "            \"injection_times\": injection_times,   # (n_events,)\n",
    "            \"injection_doses\": injection_doses,   # (n_events,)\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch 내의 각 sample의 t, injection_times 등이 모두 동일한 길이라고 가정.\n",
    "    \"\"\"\n",
    "    static_features = torch.stack([b[\"static_features\"] for b in batch], dim=0)  # (B, 7)\n",
    "    t = batch[0][\"t\"]  # (T,)\n",
    "    cp = torch.stack([b[\"cp\"] for b in batch], dim=0)  # (B, T, 1)\n",
    "    injection_times = torch.stack([b[\"injection_times\"] for b in batch], dim=0)  # (B, n_events)\n",
    "    injection_doses = torch.stack([b[\"injection_doses\"] for b in batch], dim=0)  # (B, n_events)\n",
    "    return {\n",
    "        \"static_features\": static_features,\n",
    "        \"t\": t,\n",
    "        \"cp\": cp,\n",
    "        \"injection_times\": injection_times,\n",
    "        \"injection_doses\": injection_doses,\n",
    "    }\n",
    "\n",
    "\n",
    "#############################\n",
    "# 2. 모델 정의 (Neural ODE)\n",
    "#############################\n",
    "class ODEF(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural ODE에서 사용될 ODE 함수.\n",
    "    입력으로 현재 상태(state), 정적 임베딩(static_embed)와\n",
    "    t시점에서의 투여 이벤트(투여 이벤트는 injection_times, injection_doses로 주어짐)를 반영하여\n",
    "    미분값(dstate/dt)을 계산.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, static_dim, dose_dim=1):\n",
    "        super(ODEF, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + static_dim + dose_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, t, state, static_embed, injection_times, injection_doses, sigma=1.0\n",
    "    ):\n",
    "        # t는 스칼라 텐서, injection_times: (B, n_events), injection_doses: (B, n_events)\n",
    "        # 각 배치별로 t와 injection_times의 차이를 계산하여 가우시안 커널로 근사한 투여값 계산\n",
    "        # dose: (B, 1)\n",
    "        dose = (\n",
    "            injection_doses * torch.exp(-((t - injection_times) ** 2) / (2 * sigma**2))\n",
    "        ).sum(dim=1, keepdim=True)\n",
    "        dose = torch.clamp(dose, min=0.0, max=1e2)\n",
    "        inp = torch.cat([state, static_embed, dose], dim=-1)\n",
    "        return self.net(inp)\n",
    "\n",
    "\n",
    "class NeuralODEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    전체 모델:\n",
    "    - 정적 특성은 간단한 피드포워드 네트워크(static_encoder)로 임베딩.\n",
    "    - 초기 상태 h0는 static_embed로부터 얻으며,\n",
    "    - ODE 함수(ODEF)를 torchdiffeq.odeint로 풀어 시계열 은닉상태 h(t)를 얻고,\n",
    "    - 최종 readout 레이어로 각 시점의 CP 값을 예측.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, static_input_dim=7, static_hidden_dim=16, ode_hidden_dim=16):\n",
    "        super(NeuralODEModel, self).__init__()\n",
    "        self.static_encoder = nn.Sequential(\n",
    "            nn.Linear(static_input_dim, static_hidden_dim), nn.ReLU()\n",
    "        )\n",
    "        self.initial_state_layer = nn.Linear(static_hidden_dim, ode_hidden_dim)\n",
    "        self.ode_func = ODEF(ode_hidden_dim, static_hidden_dim, dose_dim=1)\n",
    "        self.readout = nn.Linear(ode_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, t, static_features, injection_times, injection_doses):\n",
    "        \"\"\"\n",
    "        t: (T,) – 통합할 시간 벡터\n",
    "        static_features: (B, 7)\n",
    "        injection_times: (B, n_events)\n",
    "        injection_doses: (B, n_events)\n",
    "        \"\"\"\n",
    "        # 정적 특성 임베딩\n",
    "        static_embed = self.static_encoder(static_features)  # (B, static_hidden_dim)\n",
    "        # 초기 ODE 상태 h0 (배치별)\n",
    "        h0 = self.initial_state_layer(static_embed)  # (B, ode_hidden_dim)\n",
    "\n",
    "        # ODE 함수 정의 (추가 인자로 static_embed, injection_times, injection_doses 전달)\n",
    "        def func(t_val, h):\n",
    "            return self.ode_func(\n",
    "                t_val, h, static_embed, injection_times, injection_doses\n",
    "            )\n",
    "\n",
    "        # ODE 풀기 (rk4 방법 사용)\n",
    "        # odeint 출력: (T, B, ode_hidden_dim)\n",
    "        h_t = odeint(func, h0, t, method=\"rk4\")\n",
    "        h_t = h_t.transpose(0, 1)  # (B, T, ode_hidden_dim)\n",
    "        cp_pred = self.readout(h_t)  # (B, T, 1)\n",
    "        return cp_pred\n",
    "\n",
    "\n",
    "#############################\n",
    "# 3. Inference 함수\n",
    "#############################\n",
    "def run_inference(\n",
    "    model, static_features, injection_times, injection_doses, total_time, device\n",
    "):\n",
    "    \"\"\"\n",
    "    기존 모델을 불러와서 사용자 입력(정적 특성, 투여 이벤트, 총 측정시간)에 따라\n",
    "    시간에 따른 CP 값을 예측한 후, CSV 파일 및 차트 이미지(predicted_cp.png)를 생성.\n",
    "\n",
    "    인자:\n",
    "      - static_features: 리스트 또는 np.array, shape (7,)\n",
    "      - injection_times: 리스트 또는 np.array, shape (n_events,)\n",
    "      - injection_doses: 리스트 또는 np.array, shape (n_events,)\n",
    "      - total_time: 총 측정 시간 (스칼라)\n",
    "    \"\"\"\n",
    "    # 예측할 시간 벡터 (여기서는 1시간 간격으로 샘플링)\n",
    "    # 수정 후: t를 1부터 total_time까지로 설정 (총 T time steps)\n",
    "    T = int(total_time) + 1\n",
    "    t = torch.linspace(0, total_time, steps=T).to(device)\n",
    "    static_features = (\n",
    "        torch.tensor(static_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    )  # (1, 7)\n",
    "    injection_times = (\n",
    "        torch.tensor(injection_times, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    )  # (1, n_events)\n",
    "    injection_doses = (\n",
    "        torch.tensor(injection_doses, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    )  # (1, n_events)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cp_pred = model(\n",
    "            t, static_features, injection_times, injection_doses\n",
    "        )  # (1, T, 1)\n",
    "    cp_pred = cp_pred.squeeze().cpu().numpy()  # (T,)\n",
    "\n",
    "    # 예측 데이터를 CSV로 저장\n",
    "    pred_df = pd.DataFrame({\"time\": t.cpu().numpy(), \"CP_pred\": cp_pred})\n",
    "    pred_df.to_csv(\"predicted_cp.csv\", index=False)\n",
    "\n",
    "    # 차트 생성 및 저장\n",
    "    plt.figure()\n",
    "    plt.plot(t.cpu().numpy(), cp_pred, marker=\".\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"CP\")\n",
    "    plt.title(\"Predicted CP over Time\")\n",
    "    plt.savefig(\"predicted_cp.png\")\n",
    "    plt.close()\n",
    "    print(\"Inference 완료: predicted_cp.csv, predicted_cp.png 생성됨.\")\n",
    "    return cp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잘못된 입력입니다.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuserver/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# 4. Main (학습 및 Inference 실행)\n",
    "#############################\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# (이전 섹션에서 정의한 PKPDataset, collate_fn, NeuralODEModel, run_inference 등이 이미 import 또는 정의되어 있다고 가정)\n",
    "\n",
    "mode_input = input(\"모드를 선택하세요 (train, infer, test_train, test_infer 중 하나): \").strip()\n",
    "if mode_input not in [\"train\", \"infer\", \"test_train\", \"test_infer\"]:\n",
    "    print(\"잘못된 입력입니다.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.mode = mode_input\n",
    "print(f\"선택한 모드: {args.mode}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralODEModel().to(device)\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "start_epoch = 0\n",
    "loss_history = []\n",
    "\n",
    "# 체크포인트가 있으면 불러옴 (학습 이어서 진행 가능)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    loss_history = checkpoint.get(\"loss_history\", [])\n",
    "    print(f\"체크포인트 불러옴: epoch {start_epoch}부터 이어서 학습합니다.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "            print(f\"현재 사용 중인 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"사용 중인 GPU 메모리: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            print(f\"예약된 GPU 메모리: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "if args.mode in [\"train\", \"test_train\"]:\n",
    "    # 데이터셋 및 DataLoader 구성: test_train 모드이면 x행의 데이터만 사용 (= x명의 환자 데이터)\n",
    "    if args.mode == \"test_train\":\n",
    "        dataset = PKPDataset(parquet_path=\"../processed_data.parquet\", max_files=5, max_rows=30)\n",
    "    else:\n",
    "        dataset = PKPDataset(parquet_path=\"../processed_data.parquet\")\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    num_epochs = int(input(f\"몇 epoch까지 학습할까요? 현재까지 학습된 epoch 수: {start_epoch} (학습 중간에 코드가 멈추더라도, 각 epoch가 끝날 때마다 자동으로 저장됩니다.) \") or 100)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        if False: # args.mode == \"test_train\" and (epoch % 10 != 0):\n",
    "            iterator = dataloader  # tqdm 없이 진행\n",
    "        else:\n",
    "            iterator = tqdm(dataloader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        for i, batch in enumerate(iterator, start=1):\n",
    "            static_features = batch[\"static_features\"].to(device)  # (B, 7)\n",
    "            t = batch[\"t\"].to(device)  # (T,)\n",
    "            cp_target = batch[\"cp\"].to(device)  # (B, T, 1)\n",
    "            injection_times = batch[\"injection_times\"].to(device)  # (B, n_events)\n",
    "            injection_doses = batch[\"injection_doses\"].to(device)  # (B, n_events)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cp_pred = model(t, static_features, injection_times, injection_doses)  # (B, T, 1)\n",
    "            loss = criterion(cp_pred, cp_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # 100 배치마다 진행상황 출력\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {i}/{len(dataloader)}: Current batch loss = {loss.item():.6f}\", flush=True)\n",
    "\n",
    "        epoch_loss /= len(dataloader)\n",
    "        loss_history.append(epoch_loss)\n",
    "        if args.mode == \"train\" or (args.mode == \"test_train\"): # and epoch % 10 == 0):\n",
    "            print(f\"Epoch {epoch}: Average Loss = {epoch_loss:.6f}\", flush=True)\n",
    "\n",
    "        # 체크포인트 저장 (모델, 옵티마이저, epoch, loss history 포함)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss_history\": loss_history,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        # 체크포인트 저장 후, 각 epoch마다 CSV 파일 업데이트\n",
    "        loss_df = pd.DataFrame({\"epoch\": list(range(len(loss_history))), \"loss\": loss_history})\n",
    "        loss_df.to_csv(\"training_loss.csv\", index=False)\n",
    "\n",
    "        # 학습 loss 차트 그리기 및 저장\n",
    "        plt.figure()\n",
    "        plt.plot(loss_history, marker=\"o\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss (log scale)\")\n",
    "        plt.title(\"Training Loss History\")\n",
    "        plt.savefig(\"training_loss.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # 최종 loss 기록 CSV 저장\n",
    "    loss_df = pd.DataFrame({\"epoch\": list(range(len(loss_history))), \"loss\": loss_history})\n",
    "    loss_df.to_csv(\"training_loss.csv\", index=False)\n",
    "    print(\"학습 완료 및 loss 기록 저장됨.\")\n",
    "\n",
    "elif args.mode in [\"infer\", \"test_infer\"]:\n",
    "    if args.mode == \"test_infer\":\n",
    "        # 정적 특성은 사용자 입력으로 받음\n",
    "        static_input = input(\"BW, EGFR, SEX, RAAS, BPS, amt, II 값을 순서대로 입력하세요. (공백으로 구분, 예: 68.28 66.37 1 1 1 244 482): \")\n",
    "        static_features = [float(x.strip()) for x in static_input.split()]\n",
    "        # 첫 5개의 행(환자 데이터)에서 ground truth 데이터 불러오기\n",
    "        dataset = PKPDataset(parquet_path=\"../processed_data.parquet\", max_files=5)\n",
    "        gt_list = []\n",
    "        for sample in dataset:\n",
    "            gt_time = sample[\"t\"].numpy()\n",
    "            gt_cp = sample[\"cp\"].numpy().squeeze()  # (T,)\n",
    "            gt_list.append((gt_time, gt_cp))\n",
    "        print(\"첫 5개의 행의 CP 데이터가 로드되었습니다.\")\n",
    "    else:\n",
    "        static_input = input(\"BW, EGFR, SEX, RAAS, BPS, amt, II 값을 순서대로 입력하세요. (공백으로 구분, 예: 68.28 66.37 1 1 1 244 482): \")\n",
    "        static_features = [float(x.strip()) for x in static_input.split()]\n",
    "\n",
    "    total_time = float(input(\"모델이 예측할 총 시간을 입력하세요. (시간): \"))\n",
    "\n",
    "    amt = static_features[5]\n",
    "    II = static_features[6]\n",
    "    injection_times = np.arange(0, total_time + 1e-3, II, dtype=np.float32)\n",
    "    injection_doses = np.full(injection_times.shape, amt, dtype=np.float32)\n",
    "\n",
    "    pred_cp = run_inference(model, static_features, injection_times, injection_doses, total_time, device)\n",
    "\n",
    "    if args.mode == \"test_infer\":\n",
    "        # 예측된 CP와 동일한 시간 벡터 생성\n",
    "        T_pred = len(pred_cp)\n",
    "        pred_time = np.linspace(0, total_time, num=T_pred)\n",
    "        plt.figure()\n",
    "        # 첫 5개 행의 ground truth 데이터를 total_time 이하, 예측 길이에 맞게 자른 후 모두 플롯\n",
    "        for idx, (gt_time, gt_cp) in enumerate(gt_list):\n",
    "            mask = gt_time <= total_time\n",
    "            gt_time_trim = gt_time[mask]\n",
    "            gt_cp_trim = gt_cp[mask]\n",
    "            gt_time_final = gt_time_trim[:T_pred]\n",
    "            gt_cp_final = gt_cp_trim[:T_pred]\n",
    "            plt.plot(gt_time_final, gt_cp_final, label=f\"Ground Truth {idx+1}\")\n",
    "        plt.plot(pred_time, pred_cp, label=\"Predicted\", linewidth=2, color=\"black\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"CP\")\n",
    "        plt.title(\"Test Inference: Ground Truth vs Predicted CP\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"test_inference_comparison.png\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
